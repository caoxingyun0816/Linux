
5.27
https://github.com/vergnes/spring-social-weibo
QQ登录

5.29
微信公众号
查询触发

springboot
获取配置信息
1@value
2@Compent class @value

a. Environment 读取
所有的配置信息，都会加载到Environment实体中，因此我们可以通过这个对象来获取系统的配置，通过这种方式不仅可以获取.

@RestController
public class DemoController {
    @Autowired
    private Environment environment;

    @GetMapping(path = "show")
    public String show() {
        Map<String, String> result = new HashMap<>(4);
        result.put("env", environment.getProperty("server.port"));
        return JSON.toJSONString(result);
    }
}

b. @Value 注解方式
@Value注解可以将配置信息注入到Bean的属性

// 配置必须存在，且获取的是配置名为 app.demo.val 的配置信息
    @Value("${app.demo.val}")
    private String autoInject;


c. 对象映射方式
上面的两种方式对于某几个特别的配置来说，一个一个的写还好，

@Data
@Component
@ConfigurationProperties(prefix = "app.proper")
public class ProperBean {
    private String key;
    private Integer id;
    private String value;
}

上面的写法，含义是将配置文件中配置 app.proper.key, app.proper.id, app.proper.value三个配置的值，赋值给上面的bean

即通过注解ConfigurationProperties来制定配置的前缀
通过Bean的属性名，补上前缀，来完整定位配置信息的Key，并获取Value赋值给这个Bean
上面这个过程，配置的注入，从有限的经验来看，多半是反射来实现的，所以这个Bean属性的Getter/Setter方法得加一下，上面借助了Lombok来实现，标一个@Component表示这是个Bean，托付给Spring的ApplicationConttext来管理

配置优先级问题
如何读取其他配置文件如 xxx.properties 的配置信息


问题
1流程
拦截器 --> 认证服务器 --> 返回信息 --> 封装认证信息、
反射

SpringSocail

  Ctrl+o 便可快速得到父类中的所有方法
  然后勾选继承

 思考，如果是你，你需要什么
 你会要什么功能


考虑不完全
    public boolean isAllFieldNull(Object o) {
        Class cls = (Class) o.getClass();
        Field[] fields = cls.getDeclaredFields();
        boolean flag = true;
        for (Field field : fields) {
            field.setAccessible(true);
            try {
                Object val = field.get(o);
                if (val != null) {
                    if (field.getType() == String.class && "".equals(val.toString())) {
                        flag = true;
                    } else {
                        flag = false;
                        break;
                    }
                }
            } catch (Exception e) {
                throw new BusinessRunTimeException("获取属性值失败");
            }
        }
        return flag;
    }


     String ids[] = {logisticsDetail.getLogisticsId()};
            LogisticsVO logisticsVO = logisticsService.getLogistics(ids).getData().get(0);



已读未读
永远满足不了

5.31
or
or
or
go home 
or go home
go home
go home
go home
go home
go home
go home
go home
go home
cxy
go home



6.3
apiAdapter.setConnectionValues(getApi(), values)
goooooooogle
框架里的传参，基本都是放在构造函数中，直接在构造函数中初始化，调用处理方法。

jsonfarmat
时区

6.4
zuul请求超时

已读未读
每次点击时设置读取时间
然后表中存入对应id，查询对应的更新时间
、比较读取时间和更新时间
如果读取时间大于更新时间 已读
反之，未读

只需两个接口
1.点击查看时，设置每次的读取时间
2.查看列表返回读取状态

消息系统

通过用户的注册或者绑定将用户id和第三方用户信息绑定起来。

sz -y
 sz：将选定的文件发送（send）到本地机器
 rz：运行该命令会弹出一个文件选择窗口，从本地选择文件上传到服务器(receive)


数据权限
threadLocal 用户id没用清除
导致权限混乱，
和品牌有关系的数据也会丢失。
所以要将threadlocal的数据清除掉。

6.10
apt install lrzsz

linux 使用 rz 和 sz 命令
编译安装
root 账号登陆后，依次执行以下命令：
tar zxvf lrzsz-0.12.20.tar.gz
cd lrzsz-0.12.20
./configure
make
make install
上面安装过程默认把lsz和lrz安装到了/usr/local/bin/目录下，现在我们并不能直接使用，下面创建软链接，并命名为rz/sz：
cd /usr/bin
ln -s /usr/local/bin/lrz rz
ln -s /usr/local/bin/lsz sz


sz命令发送文件到本地：
# sz filename
rz命令本地上传文件到服务器：
# rz
执行该命令后，在弹出框中选择要上传的文件即可

解绑工作
17681136950
支付宝
QQ已换绑
360
京东
网易云
百度
虎扑
知乎
银行卡
招商 民生 工商 浦发
抖音
小米账号
12306
斗鱼
虎牙
慕课网 微信绑定
CSDN
学信网

微信 cxymiss256 注销中 不需要原手机的验证码
头条 注销中
微博 注销中 14天 6.13

6.12
数据库权限
数据库备份
幸运之家 兴趣培训软件

6.13
多交流

海运才有港口

6.14
月底注销
护照
居住证


6.17
一天一首

6.18
人道至尊
大圣传
罗浮

6.19
要想解剖一个类,必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法.所以先要获取到每一个字节码文件对应的Class类型的对象.

反射就是把java类中的各种成分映射成一个个的Java对象
需要获取类对应的字节码文件
然后获取到对应的Class对象
Class对象的由来是将class文件读入内存，并为之创建一个Class对象。

当 new Student()时
JVM从本地磁盘中加载Student.class
读入内存中，放在堆内存中
同时创建该类对应的Class对象
该对像包含了Class所有的信息。
同一个类的class对象只会创建一次。
Class 类的实例表示正在运行的 Java 应用程序中的类和接口也就是jvm中有N多的实例每个类都有该Class对象。
Class 没有公共构造方法。Class 对象是在加载类时由 Java 虚拟机以及通过调用类加载器中的defineClass 方法自动构造的。也就是这不需要我们自己去处理创建，JVM已经帮我们创建好了。

1、获取Class对象的三种方式
1.1 Object ——> getClass();
1.2 任何数据类型（包括基本数据类型）都有一个“静态”的class属性
1.3 通过Class类的静态方法：forName（String  className）(常用)

private 修饰方法 只能在本类访问，其他类不访问

getType 获取对应的class对象。表示类型

class getName 获取class的名(包含包的全路径)
getTypeName 类型的名字

数组比较特殊
int[] intArray = new int[]{1,2};
Class intClass = intArray.getClass();

intClass.getName() --> [I
intClass.getTypeName() --> int[]

//获取数组的元素类型
intClass.getComponentType().getName(); int
intClass.getComponentType().getTypeName(); int

数组类型
数组类型：数组本质是一个对象，所以它也有自己的类型。
例如对于int[] intArray，数组类型为class [I。数组类型中的[个数代表数组的维度，例如[代表一维数组，[[代表二维数组；[后面的字母代表数组元素类型，I代表int，一般为类型的首字母大写(long类型例外，为J)。

class [B    //byte类型一维数组
class [S    //short类型一维数组
class [I    //int类型一维数组
class [C    //char类型一维数组
class [J    //long类型一维数组，J代表long类型，因为L被引用对象类型占用了
class [F    //float类型一维数组
class [D    //double类型一维数组
class [Lcom.dada.Season    //引用类型一维数组
class [[Ljava.lang.String  //引用类型二维数组

//获取一个变量的类型
Class<?> c = field.getType();
//判断该变量是否为数组
if (c.isArray()) {
   //获取数组的元素类型
   c.getComponentType()
}

创建和初始化数组
Java反射为我们提供了java.lang.reflect.Array类用来创建和初始化数组。

//创建数组， 参数componentType为数组元素的类型，后面不定项参数的个数代表数组的维度，参数值为数组长度
Array.newInstance(Class<?> componentType, int... dimensions)

//设置数组值，array为数组对象，index为数组的下标，value为需要设置的值
Array.set(Object array, int index, int value)

//获取数组的值，array为数组对象，index为数组的下标
Array.get(Object array, int index)

例子,用反射创建 int[] array = new int[]{1, 2}
Object array = Array.newInstance(int.class, 2);
Array.setInt(array , 0, 1);
Array.setInt(array , 1, 2);

反射的缺点
没有任何一项技术是十全十美的，Java反射拥有强大功能的同时也带来了一些副作用。


性能开销
反射涉及类型动态解析，所以JVM无法对这些代码进行优化。因此，反射操作的效率要比那些非反射操作低得多。我们应该避免在经常被执行的代码或对性能要求很高的程序中使用反射。

安全限制
使用反射技术要求程序必须在一个没有安全限制的环境中运行。如果一个程序必须在有安全限制的环境中运行，如Applet，那么这就是个问题了。

内部曝光
由于反射允许代码执行一些在正常情况下不被允许的操作（比如访问私有的属性和方法），所以使用反射可能会导致意料之外的副作用－－代码有功能上的错误，降低可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可能也随着变化。


list - map
还需深入
threadLocal
每个线程拥有自己独立的ThreadLocals变量（指向ThreadLocalMap对象 ）
思考哟

缓存
高并发 高性能 分布式 权限 单点登录

 redis 实际上是个单线程工作模型

 redis 支持复杂的数据结构
redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。

redis 原生支持集群模式
在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。

由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis，虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。

list 是有序列表，这个可以玩儿出很多花样。

比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。

比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。

# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。


edis 都有哪些数据类型？分别在哪些场景下使用比较合适？

redis 主要有以下几种数据类型：

string
hash
list
set
sorted set

string
这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。

hash
这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。

list
list 是有序列表，这个可以玩儿出很多花样。

比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。

比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。

# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。
lrange mylist 0 -1
比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。


set
set 是无序集合，自动去重。

直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 redis 进行全局的 set 去重。

可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。

把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。


sorted set
sorted set 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。

Redis为什么性能那么高
因为他还是基于内存操作的

啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。redis 主要是基于内存来进行高性能、高并发的读写操作的。
那既然内存是有限的，比如 redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。

数据明明过期了，怎么还占用着内存？
这是由 redis 的过期策略来决定。

redis 过期策略
redis 过期策略是：定期删除+惰性删除。

所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。

假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。

但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。

定期删除  随机删除一些过期的数据 性能消耗太大
惰性删除 每次查询时，先判断是否过期，过期了就删除，删除后，返回空。

但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？

内存淘汰机制
redis 内存淘汰机制有以下几个：
noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。
allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。
allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。
volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。
volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。
volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。

class LRUCache<K, V> extends LinkedHashMap<K, V> {
    private final int CACHE_SIZE;

    /**
     * 传递进来最多能缓存多少数据
     *
     * @param cacheSize 缓存大小
     */
    public LRUCache(int cacheSize) {
        // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。
        super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true);
        CACHE_SIZE = cacheSize;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
        // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。
        return size() > CACHE_SIZE;
    }
}


redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS(每秒查询数)，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。

如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。

redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。

Redis 主从架构
单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。

redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发

redis replication 的核心机制
redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；
一个 master node 是可以配置多个 slave node 的；
slave node 也可以连接其他的 slave node；
slave node 做复制的时候，不会 block master node 的正常工作；
slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；
slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。

如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。

当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。

如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。

主从复制的断点续传
从 redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。

master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization。

过期 key 处理
slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。

但是，如果 master node 死掉了，会怎么样？没法写数据了，写缓存的时候，全部失效了。slave node 还有什么用呢，没有 master 给它们复制数据了，系统相当于不可用了。

redis 的高可用架构，叫做 failover 故障转移，也可以叫做主备切换。

master node 在故障时，自动检测，并且将某个 slave node 自动切换位 master node的过程，叫做主备切换。这个过程，实现了 redis 的主从架构下的高可用。

哨兵至少需要 3 个实例，来保证自己的健壮性。
哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。

redis 哨兵主备切换的数据丢失问题

主备切换的过程，可能会导致数据丢失：

异步复制导致的数据丢失 因为 master->slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。

脑裂导致的数据丢失 脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。

此时虽然某个 slave 被切换成了master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。

redis 如果仅仅只是将数据缓存在内存里面，如果 redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。
如果 redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。

redis 持久化的两种方式
RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。
AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集

RDB和AOF到底该如何选择
不要仅仅使用 RDB，因为那样会导致你丢失很多数据；
也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug；
redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。

redis cluster，主要是针对海量数据+高并发+高可用的场景。redis cluster 支撑 N 个 redis master node，每个 master node 都可以挂载多个 slave node。这样整个 redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。

redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。

机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。

5 台机器对外提供读写，一共有 50g 内存。

因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。

你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。

其实大型的公司，会有基础架构的 team 负责缓存集群的运维。

redis 的并发竞争问题是什么？如何解决这个问题？了解 redis 事务的 CAS 方案吗？这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。

而且 redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。

某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。



分库分表
分表
比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。

分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。

分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。

这就是所谓的分库分表，为啥要分库分表？你明白了吧。

mysql读写分离
高并发这个阶段，肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是 app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？

其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。

主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。

这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。

而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。

这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。

所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。

MySQL 主从同步延时问题（精华）
以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。

是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。

我们通过 MySQL 命令：

show status
查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。

一般来说，如果主从延迟较为严重，有以下解决方案：

分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。



ElasticsSearch

我们建立一个网站或应用程序，并要添加搜索功能，但是想要完成搜索工作的创建是非常困难的。我们希望搜索解决方案要运行速度快，我们希望能有一个零配置和一个完全免费的搜索模式，我们希望能够简单地使用JSON通过HTTP来索引数据，我们希望我们的搜索服务器始终可用，我们希望能够从一台开始并扩展到数百台，我们要实时搜索，我们要简单的多租户，我们希望建立一个云的解决方案。因此我们利用Elasticsearch来解决所有这些问题及可能出现的更多其它问题

ELK

elasticsearch+Logstash+kibana 

elasticsearch：后台分布式存储以及全文检索 
logstash: 日志加工、“搬运工” 
kibana：数据可视化展示。 
ELK架构为数据分布式存储、可视化查询和日志解析创建了一个功能强大的管理链。 三者相互配合，取长补短，共同完成分布式大数据处理工作。

如果你已经有一个在运行的复杂的系统，你的需求之一是在现有系统中添加检索服务。一种非常冒险的方式是重构系统以支持ES。而相对安全的方式是：将ES作为新的组件添加到现有系统中。 
如果你使用了如下图所示的SQL数据库和ES存储，你需要找到一种方式使得两存储之间实时同步。需要根据数据的组成、数据库选择对应的同步插件。可供选择的插件包括： 
1）mysql、oracle选择 logstash-input-jdbc 插件。 
2）mongo选择 mongo-connector工具。

es 写数据过程
客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。
coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。
实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。
coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。

es 读数据过程
可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。

客户端发送请求到任意一个 node，成为 coordinate node。
coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。
接收请求的 node 返回 document 给 coordinate node。
coordinate node 返回 document 给客户端。

你根据 java 关键词来搜索，将包含 java的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。

客户端发送请求到一个 coordinate node。
协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard，都可以。
query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。
fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。

删除/更新数据底层原理
如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。

如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。

倒排索引
在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。

那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。

举个栗子。

有以下文档：

DocId   Doc
1   谷歌地图之父跳槽 Facebook
2   谷歌地图之父加盟 Facebook
3   谷歌地图创始人拉斯离开谷歌加盟 Facebook
4   谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关
5   谷歌地图之父拉斯加盟社交网站 Facebook
对文档进行分词之后，得到以下倒排索引。

WordId  Word    DocIds
1   谷歌  1,2,3,4,5
2   地图  1,2,3,4,5
3   之父  1,2,4,5
4   跳槽  1,4
5   Facebook    1,2,3,4,5
6   加盟  2,3,5
7   创始人 3
8   拉斯  3,5
9   离开  3
10  与   4
..  ..  ..
另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。

那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。

要注意倒排索引的两个重要细节：

倒排索引中的所有词项对应一个或多个文档；
倒排索引中的词项根据字典顺序升序排列。


这个问题是肯定要问的，说白了，就是看你有没有实际干过 es，因为啥？其实 es 性能并没有你想象中那么好的。很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下 5~10s，坑爹了。第一次搜索的时候，是 5~10s，后面反而就快了，可能就几百毫秒。

es 的搜索引擎严重依赖于底层的 filesystem cache，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。

性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒、5秒、10秒。但如果是走 filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。

这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G。每台机器给 es jvm heap 是 32G，那么剩下来留给 filesystem cache 的就是每台机器才 32G，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T，那么每台机器的数据量是 300G。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。

归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。

根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。

其实可以做数据预热。

举个例子，拿微博来说，你可以把一些大V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快

或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。

对于那些你觉得比较热的，经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，一定性能会好一些。


冷热分离
es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。

你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。

document 模型设计
对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。

最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。

document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就是那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。


es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。
我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。
目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个shard。


6.24
都强的一批 hard
数据库支撑到每秒并发两三千的时候，基本就快完了

高并发
Redis
ES
分库分表
读写分离
MQ
系统拆分

会使用技术不是难点
理解原理
结合业务逻辑
什么数据使用不同高并发技术处理才是重点


分布式
dubbo
如果不拆分系统，开发效率极其低下
系统耦合太大
比如说将电商系统拆分成订单系统、商品系统、采购系统、仓储系统、用户系统，等等吧。
核心意思就是根据情况，先拆分一轮，后面如果系统更复杂了，可以继续分拆

微服务
超时重试、负载均衡

重试机制

实际

解绑工作
17681136950
支付宝
QQ已换绑
360
京东
网易云
百度
虎扑
知乎
银行卡
招商 民生 工商 浦发
抖音
小米账号
12306
斗鱼
虎牙
慕课网 微信绑定
CSDN
学信网
头条 
微博

微信 cxymiss256 注销中 不需要原手机的验证码

晚上就可以解绑啦啦啦啦啦啦

保持幂等性
比如支付订单重复扣款问题
在支付的时候，根据订单id判断记录，插入该订单id的记录
如果支付了并更新状态为已支付
如果相同给得请求再次进来，就要判断了，已经插入并支付的了，
就不能够再处理了

上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信心，可以用 zookeeper 来做，对吧。
然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上。
接着你就该发起一次请求了，咋发起？当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址。
然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是。
接着找到一台机器，就可以跟它发送请求了，第一个问题咋发送？你可以说用 netty 了，nio 方式；第二个问题发送啥格式数据？你可以说用 hessian 序列化协议了，或者是别的，对吧。然后请求过去了。
服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。

一幕锥心的结局
像呼吸一样无法停息

我知道你我都没有错
只是忘了怎么退后

zookeeper

分布式协调
分布式锁
元数据/配置信息管理
HA高可用性

举个栗子。对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。

这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。

zk 分布式锁
zk 分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。

redis 分布式锁和 zk 分布式锁的对比
redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。
zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。
另外一点就是，如果是 redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。

redis 分布式锁大家没发现好麻烦吗？遍历上锁，计算时间等等......zk 的分布式锁语义清晰实现简单。

所以先不分析太多的东西，就说这两点，我个人实践认为 zk 的分布式锁比 redis 的分布式锁牢靠、而且模型简单易用。

分布式事务什么鬼

如果你真的被问到，可以这么说，我们某某特别严格的场景，用的是 TCC 来保证强一致性；然后其他的一些场景基于阿里的 RocketMQ 来实现分布式事务。

你找一个严格资金要求绝对不能错的场景，你可以说你是用的 TCC 方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。

友情提示一下，RocketMQ 3.2.6 之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。

当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于 RocketMQ 来玩儿。

tomcat + redis
这个其实还挺方便的，就是使用 session 的代码，跟以前一样，还是基于 tomcat 原生的 session 支持即可，然后就是用一个叫做 Tomcat RedisSessionManager 的东西，让所有我们部署的 tomcat 都将 session 数据存储到 redis 即可。

spring session + redis
第一种方式会与 tomcat 容器重耦合，如果我要将 web 容器迁移成 jetty，难道还要重新把 jetty 都配置一遍？

因为上面那种 tomcat + redis 的方式好用，但是会严重依赖于web容器，不好将代码移植到其他 web 容器上去，尤其是你要是换了技术栈咋整？比如换成了 spring cloud 或者是 spring boot 之类的呢？

所以现在比较好的还是基于 Java 一站式解决方案，也就是 spring。人家 spring 基本上包掉了大部分我们需要使用的框架，spirng cloud 做微服务，spring boot 做脚手架，所以用 sping session 是一个很好的选择。


上面的代码就是 ok 的，给 sping session 配置基于 redis 来存储 session 数据，然后配置了一个 spring session 的过滤器，这样的话，session 相关操作都会交给 spring session 来管了。接着在代码中，就用原生的 session 操作，就是直接基于 spring sesion 从 redis 中获取数据了。

6.27
自己搭建
我们先安装一个文件上传和下载的工具软件包 ：yum -y install lrzsz    
lrzsz包安装完成后包括上传rz、下载sz命令
然后我们可以 输入命令 rz -y  
选取文件就可以上传了

nat模式可以上网但是不能和宿主计算机通信，理论上是更安全的，无论虚拟的系统做任何破坏，中毒，木马，最终不会影响宿主计算机。桥接模式相当于是交换机上又接了个独立主机，这个在不好的时候是会向子网中传递信号的，一般是作为子网中提供服务用的。两个用处不同而已

nat模式主机可以ping通虚拟机，虚拟机不能ping通主机。bridge模式配置好后(在同一网段)可以相互ping通


此主机支持 Intel VT-x，但 Intel VT-x 处于禁用状态。

如果已在 BIOS/固件设置中禁用 Intel VT-x，或主机自更改此设置后从未重新启动，则 Intel VT-x 可能被禁用。

 (1) 确认 BIOS/固件设置中启用了 Intel VT-x 并禁用了“可信执行”。

 (2) 如果这两项 BIOS/固件设置有一项已更改，请重新启动主机。

 (3) 如果您在安装 VMware Workstation 之后从未重新启动主机，请重新启动。

 (4) 将主机的 BIOS/固件更新至最新版本。

此主机不支持“Intel EPT”硬件辅助的 MMU 虚拟化。

模块“CPUIDEarly”启动失败。


 sc create VMAuthdService binpath= "D:\VMware\VMware Workstation\vmware-authd.exe"

 DataInterceptor

 6.27
安装虚拟机
安装Centos 7.4

1.问题
VM 12 注意一定要开启虚拟化技术
可以用360的安全设置测试。
下载centos并设置虚拟机安装centos7.4
https://blog.csdn.net/qq_39135287/article/details/83993574
账户用户密码
root
cxydy0816

2.问题记录
网络配置
未安装软件

general 481
report 481 773

773 837  717 
customer_product 
datavoc type

配置IP地址，网关
cd /etc/sysconfig/network-scripts/    //进入到network-scripts目录下  
 
vi  ifcfg-eth0  //编辑配置文件 
 
//修改以下内容
BOOTPROTO=static  //启用静态IP地址
#DEVICE=eth0     //注释掉DEVICE 或者 删除DEVICE
ONBOOT=yes      //开启自动启用网络连接
 
NAT模式
 
//添加以下内容
IPADDR=192.168.132.1      //设置IP地址
NETMASK=255.255.255.0   //子网掩码 
GATEWAY=192.168.131.2   //设置网关 同NAT设置

重启网络服务
service network restart

设置DNS地址     
vi /etc/resolv.conf    //编辑 resolv.conf文件
nameserver 114.114.114.114   //添加DNS地址

永久关闭 iptables

systemctl stop firewalld.service  // 停止firewalld服务
 
systemctl disable firewalld.service // 开机禁用firewalld服务

永久关闭SELinux防火墙

vi /etc/sysconfig/selinux       //编辑selinux文件
 
SELINUX=disabled         //把文件中的SELINUX=enforcing 改成 SELINUX=disabled 即可
 
sestatus    //查看SELinux状态

权限
chmod 777 

重启
reboot 

虚拟机网络配置
https://blog.csdn.net/zkuncn/article/details/78452098

Linux 软件安装
查看是否安装了软件
rpm -qa|grep vim

Linux /bin, /sbin, /usr/bin, /usr/sbin 区别
bin: 
bin为binary的简写主要放置一些系统的必备执行档例如:cat、cp、chmod df、dmesg、gzip、kill、ls、mkdir、more、mount、rm、su、tar等。

/usr/bin:
主要放置一些应用软件工具的必备执行档例如c++、g++、gcc、chdrv、diff、dig、du、eject、elm、free、gnome*、 zip、htpasswd、kfm、ktop、last、less、locale、m4、make、man、mcopy、ncftp、 newaliases、nslookup passwd、quota、smb*、wget等。

/sbin: 
主要放置一些系统管理的必备程序例如:cfdisk、dhcpcd、dump、e2fsck、fdisk、halt、ifconfig、ifup、 ifdown、init、insmod、lilo、lsmod、mke2fs、modprobe、quotacheck、reboot、rmmod、 runlevel、shutdown等

/usr/sbin: 
放置一些网路管理的必备程序例如:dhcpd、httpd、imap、in.*d、inetd、lpd、named、netconfig、nmbd、samba、sendmail、squid、swap、tcpd、tcpdump等

/bin: 是系统的一些指令.

/sbin: 一般是指超级用户指令.

/usr/bin: 是你在后期安装的一些软件的运行脚本.

综述：
如果是用户和管理员必备的二进制文件，就会放在/bin；
如果是系统管理员必备，但是一般用户根本不会用到的二进制文件，就会放在 /sbin。
如果不是用户必备的二进制文件，多半会放在/usr/bin；
如果不是系统管理员必备的工具，如网络管理命令，多半会放在/usr/sbin。

linux 创建连接命令 ln -s 软链接
这是linux中一个非常重要命令，请大家一定要熟悉。它的功能是为某一个文件在另外一个位置建立一个同不的链接，这个命令最常用的参数是-s,

具体用法是：ln -s 源文件 目标文件。

esc
:wq 退出保存：q!直接退出

ln -s /var/www/test /var/test

创建/var/test 引向/var/www/test 文件夹 

JavaMailSender
Pay
研究

yum install的时候提示：Loaded plugins: fastestmirror

fastestmirror是yum的一个加速插件，这里是插件提示信息是插件不能用了。

不能用就先别用呗，禁用掉，先yum了再说。

1.修改插件的配置文件

# vi  /etc/yum/pluginconf.d/fastestmirror.conf  

enabled = 1//由1改为0，禁用该插件

...............................

2.修改yum的配置文件

# vi /etc/yum.conf

.........................

plugins=1//改为0，不使用插件

安装

解析不了域名
vi /etc/resolv.conf
在文件中添加如下两行：
nameserver 8.8.8.8
nameserver 8.8.4.4

在ifcfg-eth0 中也需要添加
nameserver 8.8.8.8
nameserver 8.8.4.4

保存后记得一定重启服务器
如果配置都已经配置好了，仍然不想先重启下试试。。。。。

重启

安装软件
查看是否安装了软件
rpm -qa|grep vim
yum serch applicationName 

7.5
做完说
877
881
5589
5617

查阅
支付

7.9
Jenkins 启动程序
搭建ELK 日志
支付
如果建的索引

单点登录权限
三方登录
OAuth2
邮件 邮件模板
上传 文件服务器
支付接口
微信支付宝支付

7.10
Java 13 
新特性
switch (day) {
    case MONDAY, FRIDAY, SUNDAY -> System.out.println(6);
    case TUESDAY                -> System.out.println(7);
    case THURSDAY, SATURDAY     -> System.out.println(8);
    case WEDNESDAY              -> System.out.println(9);
}
int numLetters = switch (day) {
    case MONDAY, FRIDAY, SUNDAY -> 6;
    case TUESDAY                -> 7;
    case THURSDAY, SATURDAY     -> 8;
    case WEDNESDAY              -> 9;
};

java文本块不需要再转义了

String html = """
               <html>
                   <body>
                       <p>Hello World.</p>
                   </body>
               </html>
              """;

String html = """
              <html>
                  <body>
                      <p>Hello, world</p>
                  </body>
              </html>
              """;


10、使用合理的分页方式以提高分页的效率

select id,name from product limit 866613, 20

使用上述SQL语句做分页的时候，可能有人会发现，随着表数据量的增加，直接使用limit分页查询会越来越慢。

优化的方法如下：可以取前一页的最大行数的id，然后根据这个最大的id来限制下一页的起点。比如此列中，上一页最大的id是866612。SQL可以采用如下的写法：

select id,name from product where id> 866612 limit 20


git 错误提交
git reset HEAD~ --hard 撤回最近的一次提交

git reset HEAD~ --solf 撤回最近的一次提交，但保留改动


git reset HEAD@{number} 根据number撤回提交

git checkout trueBranch
将改动从错误的分支摘取下来放在正确的分支上
git cherry-pick falseBranch

git check falseBranch
删除错误分支上的改动
git reset HEAD~ --hard   


0712
采购用途   purchase_use:对不上的，默认  特批采购

销售渠道：全渠道共用

所属类目：国内采购

计划销售月份：1

收货方式：采购单-采购到货单——仓库

存货编码：一单中有重复行时，数量合并，价格加权平均。

供应商账期：？默认账期；如无？

预估销量=采购数量demand

下单数量=order_quantity

状态为“已下单”；

select * from supplier where unique_code = 'GYS115983'

111
当数据量多的时候必须给字段建立索引
给子表和主表联合查询需要给关联条件建立索引

7.12
采购去掉按部门过滤
按品牌

7.15
服务之间feign超时问题
设置参数
ribbon:
  ReadTimeout: 20000 读取超时时间
  ConnectTimeout: 3000 连接超时时间

springboot log系统
Spring Boot默认使用LogBack日志系统，如果不需要更改为其他日志系统如Log4j2等，则无需多余的配置，LogBack默认将日志打印到控制台上。

但是因为新建的Spring Boot项目一般都会引用spring-boot-starter或者spring-boot-starter-web，而这两个起步依赖中都已经包含了对于spring-boot-starter-logging的依赖，所以，无需额外添加依赖。
Spring Boot默认的日志级别为INFO

很多开发者在日常写private static final Logger LOG = LoggerFactory.getLogger(LogConfig.class);总觉得后面的LogConfig.class可有可无，因为随便写个其他类也不会报错，但是准确编写class信息能够提供快速定位日志的效率。

我们看到打印的日志内容左侧就是对应的类名称，这个是通过private static final Logger LOG = LoggerFactory.getLogger(LogConfig.class);实现的。

日志级别总共有TARCE < DEBUG < INFO < WARN < ERROR < FATAL ，且级别是逐渐提供，如果日志级别设置为INFO，则意味TRACE和DEBUG级别的日志都看不到。

上例中我们打印了一个INFO级别的日志，因为Spring Boot默认级别就是INFO，如果我们改为WARN，是否还能看到这行日志信息。

logging.level

该属性用于配置日志级别。

在applicaition.properties中添加

logging.level.root=warn

设置日志级别
紧急程度越低，日志越多，包含日志比他少的级别
越少越紧急

日志
由多到少
级别越高

changeRepository(repositoryVOS,purchase.getRepository())
List<U8RepositoryVO> repositoryVOS

采购数据一条条保存


current_timestamp(3)


update purchase set update_user = '92b2e2d134d74f16b045a9f6a41e23d9' where update_user = '张敬1'

update purchase set update_time = '2019-07-12 13:24:05.063' where update_user = '92b2e2d134d74f16b045a9f6a41e23d9'

insert into puchase_time (sss,updatetime) select id , update_time from purchase

select * from  puchase_time ORDER BY  updatetime desc

select a.create_user ,b.* from purchase a left join Sheet2 b on a.create_user = b.employeeId 

select * from Sheet2

UPDATE purchase a
INNER JOIN (
    SELECT
        departmentId AS departmentId,
        employeeId  AS user_id
    FROM
        Sheet2
) b
SET a.department_id = b.departmentId
WHERE
     a.create_user = b.user_id;
     
     select count(*) from purchase
     
      select * from purchase where 
     
     UPDATE purchase a
INNER JOIN (
    SELECT
        updatetime AS updatetime,
        sss  AS id
    FROM
        puchase_time
) b
SET a.update_time = b.updatetime
WHERE
     a.id = b.id;


5e9432a187af46ce899cbbdbb425dc01
d5799eea7f3a4d3990c145aff91d890b

brand [{"fieldName":"id","fieldValue":["0e165087625d11e995a70862664c73a4","4a9c491f18a211e9951d0862664c73a4"],"operator":"in"}]
brand_side_project [{"fieldName":"id","fieldValue":["f872089b18a211e9951d0862664c73a4","8e3c982a18a211e9951d0862664c73a4"],"operator":"in"}]

DataAuthority:5e9432a187af46ce899cbbdbb425dc01

bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpYXQiOjE1NjMyNzAyODksIm5iZiI6MTU2MzI3MDI4OSwiZXhwIjoxNTYzMzU2OTg5LCJ1c2VyX25hbWUiOiI1ZTk0MzJhMTg3YWY0NmNlODk5Y2JiZGJiNDI1ZGMwMSIsImF1dGhvcml0aWVzIjoiMjUwOjQ3MyJ9.mugDChP7_IXllDQih3mg1hhXCgTOO95hpEBBXFAcpBIoMtpD10NXT_CIVGFxK3nSe43g0OIfDKZHDQKfwUAzPkqa3d74UGKsXQoj8u4cTxM_Me3AQFUkI7hj2rZznhfY2IlGwsj6KfSkR_Ydz_3T55k4M1X-Z_OiB55M9Uhnz7kcfQZxfrIvlbFgC4BgVKbjVluOylRuAwGDOPAMpf3lBTOc6I97pi0omtTse22JHopgoR0E-Zm6FmjOs4eHVqKaAfn5mpmMpdNTKQjaK9hYLeXXiceqmnUAOjktW4Oz3Ek_mAq6TcOYE19cdGRco7kl3O1SQl_G1apAWePAFeGB-Q

brand [{"fieldName":"id","fieldValue":["081a1027543811e9bf84005056bc3068","0b97f140972311e9bf84005056bc3068","0e165087625d11e995a70862664c73a4"],"operator":"in"}]

DataAuthority:5e9432a187af46ce899cbbdbb425dc01

brand


数据权限操作权限邮件提醒支付
先把系统的研究透自己好好钻研

7.17
log4j2 配置文件

log4j是apache实现的一个开源日志组件
logback同样是由log4j的作者设计完成的，拥有更好的特性，用来取代log4j的一个日志框架，是slf4j的原生实现
Log4j2是log4j 1.x和logback的改进版，据说采用了一些新技术（无锁异步、等等），使得日志的吞吐量、性能比log4j 1.x提高10倍，并解决了一些死锁的bug，而且配置更加简单灵活
然后，了解一下，使用slf4j+log4j和直接用log4j的区别
slf4j是对所有日志框架制定的一种规范、标准、接口，并不是一个框架的具体的实现，因为接口并不能独立使用，需要和具体的日志框架实现配合使用（如log4j、logback），使用接口的好处是当项目需要更换日志框架的时候，只需要更换jar和配置，不需要更改相关java代码

<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="debug" monitorInterval="30">
    <Properties>
        <Property name="log.path">${sys:logPath}</Property>
        <Property name="defaultLevel">${sys:defaultLevel}</Property>
        <Property name="logstashUrl">${sys:logstashUrl}</Property>
    </Properties>

    <Appenders>
        <console name="Console" target="SYSTEM_OUT">
            <PatternLayout pattern="[%d{HH:mm:ss:SSS}] [%p] - %l - %m%n"/>
        </console>

        <Gelf name="LogstashInfo" host="${logstashUrl}" port="7002" version="1.1" extractStackTrace="true"
              filterStackTrace="true" mdcProfiling="true" includeFullMdc="true" maximumMessageSize="8192"
              originHost="%host{fqdn}" additionalFieldTypes="fieldName1=String,fieldName2=Double,fieldName3=Long">
            <Field name="time" pattern="%d{yyyy-MM-dd HH:mm:ss,SSS}" />
            <Field name="level" pattern="%level" />
            <Field name="message" pattern="(%F:%L) - %message" />
            <Field name="server" pattern="product-service" />
        </Gelf>

        <!-- info日志 -->
        <RollingFile name="InfoRollingFile" fileName="${log.path}/info/info.log" filePattern="${log.path}/info/info_%d{yyyy-MM-dd}_%i.log">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss,SSS}:%4p %t (%F:%L) - %m%n" />
            <Policies>
                <!-- 每24小时更新一次 -->
                <TimeBasedTriggeringPolicy modulate="true" interval="1" />
            </Policies>
            <!-- 最多备份20个 -->
            <DefaultRolloverStrategy max="20" />
            <ThresholdFilter level="info" onMatch="ACCEPT" onMismatch="DENY"/>
        </RollingFile>

        <!-- error日志 -->
        <RollingFile name="ErrorRollingFile" fileName="${log.path}/error/error.log" filePattern="${log.path}/error/error_%d{yyyy-MM-dd}_%i.log">
            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss,SSS}:%4p %t (%F:%L) - %m%n" />
            <Policies>
                <!-- 按天更新一次 -->
                <TimeBasedTriggeringPolicy modulate="true" interval="1" />
            </Policies>
            <!-- 最多备份20个 -->
            <DefaultRolloverStrategy max="20" />
            <ThresholdFilter level="error" onMatch="ACCEPT" onMismatch="DENY"/>
        </RollingFile>
    </Appenders>

    <Loggers>
        <!--过滤掉spring和mybatis的一些无用的DEBUG信息-->
        <logger name="org.springframework" level="INFO"></logger>
        <logger name="org.mybatis" level="INFO"></logger>
        <logger name="springfox" level="INFO"></logger>
        <logger name="com.netflix" level="INFO"></logger>
        <logger name="org.apache" level="INFO"></logger>
        <logger name="org.dozer" level="INFO"></logger>
        <logger name="io.swagger.models.parameters.AbstractSerializableParameter" level="error"></logger>
        <Root level="${defaultLevel}">
            <AppenderRef ref="Console"/>
            <AppenderRef ref="InfoRollingFile"/>
            <AppenderRef ref="ErrorRollingFile"/>
            <AppenderRef ref="LogstashInfo"/>
        </Root>
    </Loggers>

</Configuration>

Configuration：为根节点，有status和monitorInterval等多个属性

status的值有 “trace”, “debug”, “info”, “warn”, “error” and “fatal”，用于控制log4j2日志框架本身的日志级别，如果将stratus设置为较低的级别就会看到很多关于log4j2本身的日志，如加载log4j2配置文件的路径等信息
monitorInterval，含义是每隔多少秒重新读取配置文件，可以不重启应用的情况下修改配置
Appenders：输出源，用于定义日志输出的地方 
log4j2支持的输出源有很多，有控制台Console、文件File、RollingRandomAccessFile、MongoDB、Flume 等

Console：控制台输出源是将日志打印到控制台上，开发的时候一般都会配置，以便调试

File：文件输出源，用于将日志写入到指定的文件，需要配置输入到哪个位置（例如：D:/logs/mylog.log）

RollingRandomAccessFile: 该输出源也是写入到文件，不同的是比File更加强大，可以指定当文件达到一定大小（如20MB）时，另起一个文件继续写入日志，另起一个文件就涉及到新文件的名字命名规则，因此需要配置文件命名规则 
这种方式更加实用，因为你不可能一直往一个文件中写，如果一直写，文件过大，打开就会卡死，也不便于查找日志。

fileName 指定当前日志文件的位置和文件名称
filePattern 指定当发生Rolling时，文件的转移和重命名规则
SizeBasedTriggeringPolicy 指定当文件体积大于size指定的值时，触发Rolling
DefaultRolloverStrategy 指定最多保存的文件个数
TimeBasedTriggeringPolicy 这个配置需要和filePattern结合使用，注意filePattern中配置的文件重命名规则是${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i，最小的时间粒度是mm，即分钟
ThresholdFilter属性：onMatch表示匹配设定的日志级别后是DENY还是ACCEPT，onMismatch表示不匹配设定的日志级别是DENY还是ACCEPT还是NEUTRAL
TimeBasedTriggeringPolicy指定的size是1，结合起来就是每1分钟生成一个新文件。如果改成%d{yyyy-MM-dd HH}，最小粒度为小时，则每一个小时生成一个文件
NoSql：MongoDb, 输出到MongDb数据库中

Flume：输出到Apache Flume（Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。）

Async：异步，需要通过AppenderRef来指定要对哪种输出源进行异步（一般用于配置RollingRandomAccessFile）

PatternLayout：控制台或文件输出源（Console、File、RollingRandomAccessFile）都必须包含一个PatternLayout节点，用于指定输出文件的格式（如 日志输出的时间 文件 方法 行数 等格式），例如 pattern=”%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n”

%d{HH:mm:ss.SSS} 表示输出到毫秒的时间
%t 输出当前线程名称
%-5level 输出日志级别，-5表示左对齐并且固定输出5个字符，如果不足在右边补0
%logger 输出logger名称，因为Root Logger没有名称，所以没有输出
%msg 日志文本
%n 换行
 
其他常用的占位符有：
%F 输出所在的类文件名，如Log4j2Test.java
%L 输出行号
%M 输出所在方法名
%l 输出语句所在的行数, 包括类名、方法名、文件名、行数
Loggers：日志器 
日志器分根日志器Root和自定义日志器，当根据日志名字获取不到指定的日志器时就使用Root作为默认的日志器，自定义时需要指定每个Logger的名称name（对于命名可以以包名作为日志的名字，不同的包配置不同的级别等），日志级别level，相加性additivity（是否继承下面配置的日志器）， 对于一般的日志器（如Console、File、RollingRandomAccessFile）一般需要配置一个或多个输出源AppenderRef；

每个logger可以指定一个level（TRACE, DEBUG, INFO, WARN, ERROR, ALL or OFF），不指定时level默认为ERROR

additivity指定是否同时输出log到父类的appender，缺省为true。

<Logger name="rollingRandomAccessFileLogger" level="trace" additivity="true">  
    <AppenderRef ref="RollingRandomAccessFile" />  
</Logger>
properties: 属性 
使用来定义常量，以便在其他配置的时候引用，该配置是可选的，例如定义日志的存放位置 
D:/logs



gelf
-DlogstashUrl=udp:192.168.1.132

feign 之间传递流
feign之间默认支持json不支持流


7.18
StaticLoggerBinder.class"，即所有slf4j的实现，在提供的jar包路径下，一定是有"org/slf4j/impl/StaticLoggerBinder.class"存在的

对于有些接口的扩展实现,不同的实现，由于代码中写死了类的路径，自己在实现接口的时候也要创建相同的包和类
然后再重写方法。

在开放的系统中，对某个接口的实现类进行初始化时，可以先获取所有实现了某个接口的实现类。如果该实现类唯一，则直接实例化。如果不唯一，则可以读取配置文件，按配置文件的配置进行实例化。从而达到了：定义接口 -> 给出实现类 -> 其它用户按接口规范自定义实现类 -> 配置实现类 -> 调用用户自定义实现类的目的。

idea 中查看jar包依赖关系
打开pom文件  -> 右键 diagrams --> show dey... --> ctrl f 查找重复依赖
在pom中排除掉。

光看不做假把式

Redis
支持数据持久化，可以将内存中的数据保存在磁盘中，重启可再次加载使用
支持简单的Key-Value类型的数据，同时还提供List、Set、Zset、Hash等数据结构的存储
支持数据的备份，即Master-Slave模式的数据备份
同时，我们再看下Redis有什么优势：

读速度为110000次/s，写速度为81000次/s，性能极高
具有丰富的数据类型，这个上面已经提过了
Redis所有操作都是原子的，意思是要么成功执行要么失败完全不执行，多个操作也支持事务
丰富的特性，比如Redis支持publish/subscribe、notify、key过期等